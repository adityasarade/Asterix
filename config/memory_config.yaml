# Token counting and limits
tokens:
  # Counting method
  counter: "tiktoken"  # tiktoken, approximate, character_based
  
  # Models for token counting
  tiktoken_model: "cl100k_base"  # For GPT-4 and text-embedding models
  approximate_ratio: 4  # Characters per token (fallback)
  
  # Core memory limits
  core_memory:
    block_limit: 1500      # Tokens per block before eviction
    total_limit: 8000      # Total tokens across all blocks
    summary_limit: 220     # Summary length after eviction
    preserve_recent: 200   # Always keep last N tokens in block
    
  # Response limits
  response:
    max_tokens: 500        # Maximum response length
    thinking_tokens: 100   # Reserved for internal reasoning

# Eviction policies and strategies
eviction:
  # Primary strategy
  strategy: "summarize_and_archive"  # summarize_and_archive, oldest_first, priority_based
  
  # Eviction triggers
  triggers:
    token_threshold: 0.9   # Evict when block reaches 90% of limit
    total_threshold: 0.85  # Evict when total memory reaches 85% of limit
    
  # Block priority for eviction (lower = evicted first)
  block_priorities:
    context: 1     # Most likely to be evicted (temporary)
    goals: 2       # Moderately likely to be evicted
    knowledge: 3   # Less likely to be evicted
    preferences: 4 # Rarely evicted (persistent)
    persona: 5     # Almost never evicted (core identity)
    human: 5       # Almost never evicted (core relationship)

# Summarization configuration
summarization:
  # Model settings
  model_provider: "auto"  # auto, groq, openai, same_as_agent
  
  # Summarization settings
  settings:
    temperature: 0.1       # Low temperature for consistent summaries
    max_tokens: 300        # Maximum summary length
    preserve_structure: true  # Try to maintain original structure

# Archival storage configuration
archival:
  # Storage strategy
  strategy: "semantic_chunks"  # semantic_chunks, time_based, priority_based
  
  # Chunk creation
  chunking:
    method: "semantic"     # semantic, fixed_size, sentence_based
    target_size: 1000      # Target tokens per chunk
    max_size: 2000         # Maximum tokens per chunk
    min_size: 200          # Minimum tokens per chunk
    overlap: 100           # Overlap between chunks

# Retrieval configuration
retrieval:
  # Search strategy
  strategy: "hybrid"  # semantic, keyword, hybrid
  
  # Semantic search settings
  semantic:
    default_k: 6           # Default number of results
    max_k: 20              # Maximum number of results
    score_threshold: 0.7   # Minimum similarity score
    
  # LLM-initiated search (primary)
  llm_initiated:
    enabled: true
    tool_name: "archival_memory_search"
    require_explicit_call: true